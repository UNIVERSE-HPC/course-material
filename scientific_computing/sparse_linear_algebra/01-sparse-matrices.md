---
name: Sparse Matrices
dependsOn: [
]
tags: []
attribution: 
- citation: This material has been adapted from material by Martin Robinson from the "Scientific Computing" module of the SABS RÂ³ Center for Doctoral Training.
  url: https://www.sabsr3.ox.ac.uk
  image: https://www.sabsr3.ox.ac.uk/sites/default/files/styles/site_logo/public/styles/site_logo/public/sabsr3/site-logo/sabs_r3_cdt_logo_v3_111x109.png
  license: CC-BY-4.0
- citation: This course material was developed as part of UNIVERSE-HPC, which is funded through the SPF ExCALIBUR programme under grant number EP/W035731/1 
  url: https://www.universe-hpc.ac.uk
  image: https://www.universe-hpc.ac.uk/assets/images/universe-hpc.png
  license: CC-BY-4.0
---

## Why sparse matrices

Taking advantage of any special structure in the matrix of interest is always of great
importance when designing a linear algebra algorithm/solver. Thus far we have discussed
special structures such as symmetric or positive definite matrices, but one of the most
common matrix structures in scientific computing is that of a *sparse matrix*, or a
matrix containing many zero elements. Since zeros can be ignored in many computations,
for example multiplication or addition, a sparse matrix will specify a special data
structure so that only the *non-zero* elements of the matrix are actually stored and
used in computation. Note also that a sparse matrix can itself be symmetric or positive
definite, and it is often necessary to take all these properties into account when
designing your algorithm.

The most obvious benefit of a sparse matrix is low memory usage. A dense matrix needs to
store all the elements of a matrix of size $n$, and thus the memory requirements scale
as $\mathcal{O}(n^2)$. For example, the following show the memory requirements of a
matrix of double precision numbers (taken from the excellent
[scipy-lectures](http://scipy-lectures.org/advanced/scipy_sparse/introduction.html#why-sparse-matrices)

![least squares problem](images/sparse_versus_dense.svg)

A sparse matrix only stores non-zero elements, and in many different applications this
represents a huge memory saving as matrices are often very sparse, holding only a few
non-zero elements. Some typical applications are:

- solution of partial differential equations (PDEs), such as the finite difference
  method illustrated below
- applications of graphs or networks (e.g. electrical networks, website links), where
  non-zero elements of the matrix represent edges between nodes

Note that while a sparse matrix has obvious benefits in terms of matrix multiplication,
where the zero elements can simply be ignored, direct solver algorithms such as $LU$
decomposition for the problem $Ax = b$, where $A$ is sparse, need considerably more
thought as the zeros in $A$ can have propagating effects, and there is no guarantee that
the decomposition of a $A$ or its inverse will be itself sparse, there can be a
significant amount of what is known as *fill-in* (i.e. non-zero elements where there
were zeros in the original matrix). This fact motivates a separate class of *iterative*
(as opposed to *direct*) solvers that only rely on the matrix multiplication of $A$ with
a vector, ignoring the internal sparsity structure of $A$ and only taking advantage of
the increased speed of the matrix multiplication itself. These iterative solvers will be
covered in the following chapter, but in this chapter we will focus on the practical
requirements of constructing and using sparse matrices using the `scipy.sparse` library.
